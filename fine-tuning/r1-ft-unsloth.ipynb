{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8e687a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munsloth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\unsloth\\__init__.py:93\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Fix Xformers performance issues since 0.0.25\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a568155",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 初始化环境\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mload_dotenv\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "# 初始化环境 quickstart\n",
    "load_dotenv()\n",
    "hf_token=os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "wb_token=os.getenv(\"WANDB_TOKEN\")\n",
    "\n",
    "login(hf_token)\n",
    "wandb.login(key=wb_token)\n",
    "\n",
    "run = wandb.init(\n",
    "    project='ft 7b on medical dataset',\n",
    "    job_type='training',\n",
    "    anonymous='allow'    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型加载及其分词器\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer =FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c91cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据模版\n",
    "prompt_style=\"\"\"以下是描述任务的指令，以及提供更多上下文的输入。\n",
    "请写出恰当完成该请求的回答。\n",
    "在回答之前，请仔细思考问题，并创建一个逐步的思维链，以确保回答合乎逻辑且准确，\n",
    "\n",
    "### Instruction:\n",
    "你是一位在临床推理、诊断和治疗计划方案具有专业知识的医学专家，\n",
    "请回答一下医学问题，\n",
    "\n",
    "### Question：\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\"\n",
    "\n",
    "# 测试用医学问题\n",
    "question=\"一位 61 岁的女性，有长期在咳嗽或打喷嚏等活动中发生不自主尿液流失的病史，但夜间没有漏尿。她接受了妇科检查和 Q-tip 测试。根据这些检查结果，膀胱测量（ cystometry ）最可能会显示她的残余尿量和逼尿肌收缩情况如何？\"\n",
    "\n",
    "# 设置模型为推理模型\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs1 = tokenizer([ prompt_style.format(question,\"\")] , return_tensors=\"pt\").to(\"cuda\") \n",
    "\n",
    "# 生成回答\n",
    "outputs1 = model.generate( \n",
    "    input_ids=inputs1.input_ids,\n",
    "    attention_mask=inputs1.attention_mask,# 注意力掩码，用户标记有效的输入位置\n",
    "    max_new_tokens=1200, \n",
    "    use_cache=True, \n",
    ") \n",
    "# 基模型输出的结果,微调后做一个对比\n",
    "response1 = tokenizer.batch_decode(outputs1)\n",
    "print(response1[0].split(\"### Response:\")[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集处理\n",
    "train_prompt_style=\"\"\"以下是描述任务的指令，以及提供更多上下文的输入。\n",
    "请写出恰当完成该请求的回答。\n",
    "在回答之前，请仔细思考问题，并创建一个逐步的思维链，以确保回答合乎逻辑且准确，\n",
    "\n",
    "### Instruction:\n",
    "你是一位在临床推理、诊断和治疗计划方案具有专业知识的医学专家，\n",
    "请回答一下医学问题，\n",
    "\n",
    "### Question：\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}</think>{}\"\"\"\n",
    "\n",
    "EOS_TOKEN=tokenizer.eos_token # 添加结束符标记\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots= examples[\"Complex_CoT\"] # ds上思维链列表\n",
    "    outputs=examples[\"Response\"]\n",
    "    \n",
    "    #存储格式化后文本\n",
    "    texts=[]\n",
    "    \n",
    "    for input,cot,output in zip(inputs,cots,outputs):\n",
    "        text = train_prompt_style.format(input,cot,output)+EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\":texts}\n",
    "\n",
    "# 加载数据集，并格式结构化\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\",\"zh\",split=\"train[0:500]\",trust_remote_code=True)\n",
    "dataset  =dataset.map( formatting_prompts_func,batched=True,) \n",
    "\n",
    "dataset[\"text\"][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
    "# 配置lora微调参数\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    # 需要应用LoRA的目标模块列表\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # attention相关层\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],   # FFN相关层\n",
    "    #缩放因子，更新的幅度\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized，防止过拟合，数据集越小，建议设置0.1左右\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized [all，lora_only]\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context 降低训练速度，但可以减少显存使用\n",
    "    random_state = 3407, # 用于结果复现\n",
    "    use_rslora = False,  # We support rank stabilized LoRA，降低速度，但可以显著的减少显存的使用\n",
    "    loftq_config = None, # And LoftQ，不使用量化技术，压缩模型大小\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 监督训练\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,# 并行进程数，提高cpu利用率\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,#每个GPU训练批次大小\n",
    "        gradient_accumulation_steps = 4,# 梯度累计步数，用于模拟更大的batch size\n",
    "        warmup_steps = 5,# 预热步数\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.一步处理一个batch的数据\n",
    "        max_steps = 60, # 60*2*4\n",
    "        learning_rate = 2e-4,\n",
    "        # 根据硬件支持选择的训练精度，看你是否支持bf16\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 10,# 10步一记录\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01, # 防止过拟合\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3200f2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab47b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"一位 61 岁的女性，有长期在咳嗽或打喷嚏等活动中发生不自主尿液流失的病史，但夜间没有漏尿。她接受了妇科检查和 Q-tip 测试。根据这些检查结果，膀胱测量（ cystometry ）最可能会显示她的残余尿量和逼尿肌收缩情况如何？\"\n",
    "\n",
    "# 设置模型为推理模型\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs1 = tokenizer([ prompt_style.format(question,\"\")] , return_tensors=\"pt\").to(\"cuda\") \n",
    "\n",
    "# 生成回答\n",
    "outputs1 = model.generate( \n",
    "    input_ids=inputs1.input_ids,\n",
    "    attention_mask=inputs1.attention_mask,# 注意力掩码，用户标记有效的输入位置\n",
    "    max_new_tokens=1200, \n",
    "    use_cache=True, \n",
    ") \n",
    "# 微调后模型输出的结果,\n",
    "response1 = tokenizer.batch_decode(outputs1)\n",
    "print(response1[0].split(\"### Response:\")[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并保存模型\n",
    "new_model_local =  \"DeepSeek-R1-Medical-COT-Tiny\" \n",
    "model.save_pretrained(new_model_local)\n",
    "tokenizer.save_pretrained(new_model_local)\n",
    "\n",
    "model.save_pretrained_merged ( new_model_local , tokenizer , save_method =  \"merged_16bit\" ,  ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18674cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上传模型\n",
    "new_model_online=\"ldz112/7B\"\n",
    "#上传LoRA权重和配置\n",
    "model.push_to_hub(new_model_online)\n",
    "tokenizer.push_to_hub(new_model_online)\n",
    "\n",
    "#上传合并的16bit模型\n",
    "model.push_to_hub_merged(new_model_online,tokenizer,save_method=\"merged_16bit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
