{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca34a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (280960864.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.max_memory = {0:21GB}\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self) ->None:\n",
    "        # LoRA配置\n",
    "        self.lora_r=8 # 核，控制可训练参数数量4-32\n",
    "        self.lora_alpha=16 #缩放因子，用于控制 LoRA 模块对原始模型参数的影响程度\n",
    "        self.lora_dropout=0.1 #用于在训练过程中对 LoRA 模块的输入进行随机丢弃，以防止过拟合 0.1-0.3\n",
    "        self.lora_target_modules=\"all-linear\" #指定哪些模型层的参数需要使用 LoRA 进行微调\n",
    "        #\"q_proj\",\"k_proj\", \"v_proj\",\"o_proj\", \"gate_proj\",\"up_proj\", \"down_proj\" # 前馈神经网络，多头注意力\n",
    "        \n",
    "        #模型训练配置\n",
    "        self.torch_dtype =\"bf16\" # 训练精度，fp16，bf16（187），fp32 \n",
    "        self.gradient_checkpointing = True\n",
    "        self.use_flash_attention=True # 长序列数据,推理速度，内存\n",
    "        self.seed = 666\n",
    "        \n",
    "        self.bnb_config={\n",
    "            \"load_in_4bit\"=True,\n",
    "            \"bnb_4bit_use_double_quant\"=True, # 双重量化，进一步压缩\n",
    "            \"bnb_4bit_quant_type\"=\"nf4\",# 量化类型，normal float 4-bit\n",
    "            \"bnb_4bit_compute_dtype\"=torch.bfloat16\n",
    "            \"llm_int8_threshold\":6.0 #用于指定 8 位量化中的异常值阈值\n",
    "            \"llm_int8_skip_modules\":None\n",
    "            \"llm_int8_has_fp16_weight \":False\n",
    "        }\n",
    "        \n",
    "        self.gpu_memory_threshold=24*1024*1024*1024\n",
    "        self.large_gpu_batch_size=16\n",
    "        \n",
    "        self.max_memory = {0:21GB}\n",
    "        self.offload_folder=\"offload\"\n",
    "        \n",
    "        self.optimizer_config={\n",
    "            \"lr\":2e-4,\n",
    "            \"betas\":(0.9,0.999),# Adam优化器的beta参数\n",
    "            \"eps\":1e-8,   #数值确定性小值   \n",
    "            \"weight_decay\":0.01\n",
    "        }\n",
    "        \n",
    "        \n",
    "class ModelConfig:\n",
    "    def __init__(self) ->None:\n",
    "        self.cache_dir=\"/tmp/.cache\"\n",
    "        self.temp_model_dir=\"/tmp/model\"\n",
    "        self.model_max_shard_size=\"2GB\"\n",
    "        \n",
    "        self.safe_serialization=True\n",
    "        self.save_strategy=\"no\" # steps表示按步保存\n",
    "        self.save_merged_model=False # 仅保存Lora权重\n",
    "        \n",
    "        \n",
    "class DataConfig:\n",
    "    def __init__(self) ->None: \n",
    "        self.max_seq_length=2048 # 最大序列长度\n",
    "        self.pad_to_max_length=True\n",
    "        self.truncation=True #截断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0bd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(model_name)\n",
    "\n",
    "def initialize_tokenizer(model_name)\n",
    "\n",
    "def process_datasets(train_ds,test_ds,tokenizer,data_config,accelerator)\n",
    "\n",
    "def get_fsdp_config(gradient_checkpointing:bool)-> tuple[str,dict]:-\n",
    "    \n",
    "def get_training_configuration\n",
    "\n",
    "def save_model\n",
    "def initialize_wandb\n",
    "\n",
    "def setup_model_environment\n",
    "\n",
    "def create_training_args\n",
    "\n",
    "def main(args)->None\n",
    "\n",
    "    load_dotenv()\n",
    "    training_config=TrainingConfig()\n",
    "    model_config=ModelConfig()\n",
    "    data_config = DataConfig()\n",
    "    \n",
    "    # 2初始化分布式训练环境\n",
    "    gradient_accumulation_steps=args.grad_accum_steps\n",
    "    accelerator = Accelerator(\n",
    "    )\n",
    "    local_rank = accelerator.num_processes>1\n",
    "    #bitsandbytes 量化开源库 peft主要提供rola微调\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
