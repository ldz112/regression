{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a9b88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "# from datasets import load_dataset\n",
    "# ds = load_dataset('csv',data_files=\"data/ChnSentiCorp.csv\")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "class BizDataset(Dataset):\n",
    "    def __init__(self,split):\n",
    "        self.dataset = load_from_disk(r\"D:\\Workspace\\regression\\huggingface\\dataset\")\n",
    "        if split==\"train\":\n",
    "            self.dataset=self.dataset[\"train\"]\n",
    "        elif split==\"test\":\n",
    "            self.dataset=self.dataset[\"test\"]\n",
    "        elif split==\"validation\":\n",
    "            self.dataset=self.dataset[\"validation\"]\n",
    "        else:\n",
    "            print(\"split is wrong\")\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        text = self.dataset[item][\"text\"]\n",
    "        label = self.dataset[item][\"label\"]\n",
    "        return text,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72e0ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "Embedding(21128, 768, padding_idx=0)\n"
     ]
    }
   ],
   "source": [
    "# net\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "DEVICE= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_name=\"bert-base-chinese\"\n",
    "model_name=r\"D:\\Workspace\\regression\\huggingface\\model\\bert-base-chinese\\models--bert-base-chinese\\snapshots\\c30a6ed22ab4564dc1e3b2ecbf6e766b0611a33f\"\n",
    "\n",
    "pretrained = BertModel.from_pretrained(model_name).to(DEVICE)\n",
    "print(pretrained)\n",
    "\n",
    "# 如需进行定制化，需要调整对应的输入输出，保持词向量一致\n",
    "print(pretrained.embeddings.word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cca6bb",
   "metadata": {},
   "source": [
    "```\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 输入文本\n",
    "text = \"Hello, how are you?\"\n",
    "\n",
    "# 分词并编码\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# 获取模型输出\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# 提取 [CLS] token 的隐藏状态\n",
    "cls_embedding = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "print(\"CLS embedding shape:\", cls_embedding.shape)  # 输出: (1, 768)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc78f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768,2)\n",
    "    # 前向推理运算\n",
    "    def forward(self,input_ids,attention_mask,token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,attention_mask= attention_mask,token_type_ids=token_type_ids)\n",
    "        \n",
    "        out= self.fc(out.last_hidden_state[:,0])\n",
    "        out = out.softmax(dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "715d4d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "0 0 0.6501349210739136 0.625\n",
      "0 5 0.6785931587219238 0.625\n",
      "0 10 0.5828644037246704 0.875\n",
      "0 15 0.6672710180282593 0.75\n",
      "0 20 0.6269630789756775 0.875\n",
      "0 25 0.5823624134063721 0.875\n",
      "0 30 0.5705785155296326 0.75\n",
      "0 35 0.6450259685516357 0.625\n",
      "0 40 0.5675601959228516 0.875\n",
      "0 45 0.6019430160522461 0.75\n",
      "0 50 0.5194454193115234 0.875\n",
      "0 55 0.6560109853744507 0.5\n",
      "0 60 0.5182225108146667 0.875\n",
      "0 65 0.5746229887008667 0.75\n",
      "0 70 0.5453912615776062 0.875\n",
      "0 75 0.5146540999412537 0.75\n",
      "0 80 0.622308075428009 0.625\n",
      "0 85 0.5554884672164917 0.75\n",
      "0 90 0.4001471996307373 1.0\n",
      "0 95 0.5963026285171509 0.625\n",
      "0 100 0.41106510162353516 1.0\n",
      "0 105 0.49354955554008484 0.875\n",
      "0 110 0.6354066729545593 0.625\n",
      "0 115 0.5349334478378296 0.75\n",
      "0 120 0.46713849902153015 1.0\n",
      "0 125 0.40546926856040955 1.0\n",
      "0 130 0.5157116651535034 0.875\n",
      "0 135 0.6548353433609009 0.625\n",
      "0 140 0.4921993911266327 0.875\n",
      "0 145 0.45338380336761475 0.875\n",
      "0 150 0.57278972864151 0.75\n",
      "0 155 0.5948582291603088 0.625\n",
      "0 160 0.5654757618904114 0.75\n",
      "0 165 0.46168577671051025 0.875\n",
      "0 170 0.462420254945755 1.0\n",
      "0 175 0.6068530082702637 0.625\n",
      "0 180 0.5154774188995361 0.75\n",
      "0 185 0.39524582028388977 1.0\n",
      "0 190 0.49668407440185547 0.75\n",
      "0 195 0.47550728917121887 0.875\n",
      "0 200 0.6194503903388977 0.625\n",
      "0 205 0.43773478269577026 0.875\n",
      "0 210 0.5096931457519531 0.75\n",
      "0 215 0.3381848931312561 1.0\n",
      "0 220 0.3480513095855713 1.0\n",
      "0 225 0.5344357490539551 0.75\n",
      "0 230 0.5232579708099365 0.75\n",
      "0 235 0.5173661708831787 0.75\n",
      "0 240 0.5328061580657959 0.75\n",
      "0 245 0.5624070763587952 0.75\n",
      "0 250 0.4874716103076935 0.75\n",
      "0 255 0.5585201978683472 0.625\n",
      "0 260 0.4158778488636017 1.0\n",
      "0 265 0.5186067819595337 0.875\n",
      "0 270 0.5768070816993713 0.75\n",
      "0 275 0.37046849727630615 1.0\n",
      "0 280 0.399357408285141 0.875\n",
      "0 285 0.5452207922935486 0.875\n",
      "0 290 0.36885401606559753 1.0\n",
      "0 295 0.38156259059906006 1.0\n",
      "0 300 0.4645060896873474 0.875\n",
      "0 305 0.4605060815811157 0.875\n",
      "0 310 0.5245026350021362 0.875\n",
      "0 315 0.4451446235179901 0.75\n",
      "0 320 0.4068000912666321 0.875\n",
      "0 325 0.429076611995697 0.875\n",
      "0 330 0.588477373123169 0.75\n",
      "0 335 0.6044570207595825 0.75\n",
      "0 340 0.47847118973731995 0.875\n",
      "0 345 0.3828747570514679 1.0\n",
      "0 350 0.4185678958892822 0.875\n",
      "0 355 0.4415651261806488 0.875\n",
      "0 360 0.5014330148696899 0.75\n",
      "0 365 0.6193838119506836 0.75\n",
      "0 370 0.5625078082084656 0.75\n",
      "0 375 0.7298795580863953 0.5\n",
      "0 380 0.5433692336082458 0.75\n",
      "0 385 0.5113584399223328 0.75\n",
      "0 390 0.4375789165496826 0.875\n",
      "0 395 0.47255125641822815 0.875\n",
      "0 400 0.3390892446041107 1.0\n",
      "0 405 0.5251644253730774 0.75\n",
      "0 410 0.6052265167236328 0.625\n",
      "0 415 0.5531933307647705 0.75\n",
      "0 420 0.5906541347503662 0.75\n",
      "0 425 0.4226933419704437 1.0\n",
      "0 430 0.5505207180976868 0.75\n",
      "0 435 0.3738519251346588 1.0\n",
      "0 440 0.4827691912651062 0.75\n",
      "0 445 0.6246883869171143 0.5\n",
      "0 450 0.6757094860076904 0.5\n",
      "0 455 0.49410444498062134 0.875\n",
      "0 460 0.5117365121841431 0.75\n",
      "0 465 0.4778169095516205 0.75\n",
      "0 470 0.41715022921562195 0.875\n",
      "0 475 0.38952332735061646 1.0\n",
      "0 480 0.425483763217926 0.875\n",
      "0 485 0.37373870611190796 1.0\n",
      "0 490 0.571992039680481 0.75\n",
      "0 495 0.5855444669723511 0.75\n",
      "0 500 0.3477429449558258 1.0\n",
      "0 505 0.6304898858070374 0.75\n",
      "0 510 0.5631943345069885 0.75\n",
      "0 515 0.3803694248199463 1.0\n",
      "0 520 0.6702176928520203 0.625\n",
      "0 525 0.44382181763648987 0.875\n",
      "0 530 0.6902663707733154 0.625\n",
      "0 535 0.7841117978096008 0.375\n",
      "0 540 0.6506632566452026 0.625\n",
      "0 545 0.4948085844516754 0.875\n",
      "0 550 0.5126659870147705 0.75\n",
      "0 555 0.6008622646331787 0.625\n",
      "0 560 0.5370954275131226 0.625\n",
      "0 565 0.5504409670829773 0.75\n",
      "0 570 0.4412514567375183 0.875\n",
      "0 575 0.5142359733581543 0.875\n",
      "0 580 0.3712286651134491 0.875\n",
      "0 585 0.4377084970474243 0.875\n",
      "0 590 0.429586261510849 0.875\n",
      "0 595 0.4400923252105713 0.875\n",
      "0 600 0.39040693640708923 0.875\n",
      "0 605 0.6079386472702026 0.625\n",
      "0 610 0.37802404165267944 1.0\n",
      "0 615 0.4103577136993408 0.875\n",
      "0 620 0.5170618295669556 0.75\n",
      "0 625 0.41779130697250366 1.0\n",
      "0 630 0.4225439727306366 0.875\n",
      "0 635 0.4879797697067261 0.875\n",
      "0 640 0.44582903385162354 0.875\n",
      "0 645 0.35054337978363037 1.0\n",
      "0 650 0.39536842703819275 1.0\n",
      "0 655 0.5990161895751953 0.75\n",
      "0 660 0.44195711612701416 0.875\n",
      "0 665 0.34442833065986633 1.0\n",
      "0 670 0.5431393384933472 0.75\n",
      "0 675 0.3939107656478882 1.0\n",
      "0 680 0.4908822178840637 0.875\n",
      "0 685 0.6348526477813721 0.625\n",
      "0 690 0.3922247290611267 1.0\n",
      "0 695 0.8656535148620605 0.375\n",
      "0 700 0.5771859884262085 0.75\n",
      "0 705 0.7453639507293701 0.375\n",
      "0 710 0.3646107017993927 1.0\n",
      "0 715 0.37410515546798706 1.0\n",
      "0 720 0.38763341307640076 1.0\n",
      "0 725 0.5981592535972595 0.625\n",
      "0 730 0.5594151616096497 0.75\n",
      "0 735 0.46336206793785095 0.875\n",
      "0 740 0.3982563018798828 0.875\n",
      "0 745 0.4608452320098877 0.875\n",
      "0 750 0.4161033034324646 0.875\n",
      "0 755 0.39001381397247314 1.0\n",
      "0 760 0.4432239830493927 0.875\n",
      "0 765 0.5590788722038269 0.625\n",
      "0 770 0.4912513196468353 0.875\n",
      "0 775 0.4559882879257202 0.875\n",
      "0 780 0.7592822313308716 0.5\n",
      "0 785 0.3916317820549011 0.875\n",
      "0 790 0.6524226665496826 0.5\n",
      "0 795 0.34842416644096375 1.0\n",
      "0 800 0.5435832142829895 0.75\n",
      "0 805 0.3392670154571533 1.0\n",
      "0 810 0.33155423402786255 1.0\n",
      "0 815 0.4186389446258545 0.875\n",
      "0 820 0.340817928314209 1.0\n",
      "0 825 0.5995504856109619 0.625\n",
      "0 830 0.6553364396095276 0.625\n",
      "0 835 0.32078269124031067 1.0\n",
      "0 840 0.40799790620803833 0.875\n",
      "0 845 0.34491854906082153 1.0\n",
      "0 850 0.486542284488678 0.875\n",
      "0 855 0.5117627382278442 0.75\n",
      "0 860 0.3350244462490082 1.0\n",
      "0 865 0.3831459581851959 0.875\n",
      "0 870 0.5093018412590027 0.75\n",
      "0 875 0.4297250509262085 0.875\n",
      "0 880 0.5745598077774048 0.75\n",
      "0 885 0.6096797585487366 0.75\n",
      "0 890 0.40176406502723694 1.0\n",
      "0 895 0.5918101668357849 0.75\n",
      "0 900 0.42926356196403503 0.75\n",
      "0 905 0.56105637550354 0.75\n",
      "0 910 0.5786726474761963 0.75\n",
      "0 915 0.4749908745288849 0.875\n",
      "0 920 0.44209933280944824 0.875\n",
      "0 925 0.6498396396636963 0.5\n",
      "0 930 0.35821419954299927 1.0\n",
      "0 935 0.5271759033203125 0.75\n",
      "0 940 0.590368926525116 0.625\n",
      "0 945 0.5077052712440491 0.75\n",
      "0 950 0.7158855199813843 0.5\n",
      "0 955 0.33127355575561523 1.0\n",
      "0 960 0.3520250618457794 1.0\n",
      "0 965 0.5375123620033264 0.75\n",
      "0 970 0.5817225575447083 0.625\n",
      "0 975 0.5082575678825378 0.75\n",
      "0 980 0.5761003494262695 0.75\n",
      "0 985 0.535196840763092 0.75\n",
      "0 990 0.383261501789093 0.875\n",
      "0 995 0.5510666370391846 0.75\n",
      "0 1000 0.6542884111404419 0.625\n",
      "0 1005 0.5441442131996155 0.75\n",
      "0 1010 0.3281852602958679 1.0\n",
      "0 1015 0.39985716342926025 0.875\n",
      "0 1020 0.5746932029724121 0.75\n",
      "0 1025 0.4939854145050049 0.875\n",
      "0 1030 0.5488873720169067 0.75\n",
      "0 1035 0.3354300856590271 1.0\n",
      "0 1040 0.4537300169467926 0.875\n",
      "0 1045 0.6110770106315613 0.625\n",
      "0 1050 0.614948034286499 0.75\n",
      "0 1055 0.4753938913345337 0.875\n",
      "0 1060 0.5683888792991638 0.75\n",
      "0 1065 0.4996456801891327 0.875\n",
      "0 1070 0.4900425970554352 0.875\n",
      "0 1075 0.45398375391960144 0.875\n",
      "0 1080 0.4891839921474457 0.875\n",
      "0 1085 0.6194593906402588 0.75\n",
      "0 1090 0.5543580055236816 0.75\n",
      "0 1095 0.5352360010147095 0.875\n",
      "0 1100 0.5366860032081604 0.75\n",
      "0 1105 0.5362699031829834 0.75\n",
      "0 1110 0.5078700184822083 0.875\n",
      "0 1115 0.5972830057144165 0.625\n",
      "0 1120 0.48239946365356445 0.875\n",
      "0 1125 0.5457919836044312 0.75\n",
      "0 1130 0.43292558193206787 0.875\n",
      "0 1135 0.4560663402080536 0.75\n",
      "0 1140 0.4974357783794403 0.875\n",
      "0 1145 0.3907390236854553 0.875\n",
      "0 1150 0.44608813524246216 0.75\n",
      "0 1155 0.44615882635116577 0.875\n",
      "0 1160 0.5944499373435974 0.625\n",
      "0 1165 0.6467729806900024 0.625\n",
      "0 1170 0.32670509815216064 1.0\n",
      "0 1175 0.4812038838863373 0.875\n",
      "0 1180 0.35838598012924194 1.0\n",
      "0 1185 0.3522219955921173 1.0\n",
      "0 1190 0.5022057890892029 0.875\n",
      "0 1195 0.5778740048408508 0.75\n",
      "0 参数保存成功。\n",
      "1 0 0.38077688217163086 1.0\n",
      "1 5 0.5229320526123047 0.75\n",
      "1 10 0.40029454231262207 0.875\n",
      "1 15 0.5846109390258789 0.75\n",
      "1 20 0.43502745032310486 0.875\n",
      "1 25 0.3791630268096924 0.875\n",
      "1 30 0.4324471950531006 0.875\n",
      "1 35 0.43626847863197327 0.875\n",
      "1 40 0.4492977559566498 0.875\n",
      "1 45 0.39405718445777893 1.0\n",
      "1 50 0.4684795141220093 0.875\n",
      "1 55 0.3392050266265869 1.0\n",
      "1 60 0.5684958100318909 0.75\n",
      "1 65 0.4694768786430359 0.875\n",
      "1 70 0.5668169856071472 0.75\n",
      "1 75 0.7590407729148865 0.5\n",
      "1 80 0.5978463888168335 0.75\n",
      "1 85 0.42101234197616577 0.875\n",
      "1 90 0.36391571164131165 1.0\n",
      "1 95 0.4435822665691376 0.875\n",
      "1 100 0.3759042024612427 1.0\n",
      "1 105 0.5157462954521179 0.875\n",
      "1 110 0.4988527297973633 0.75\n",
      "1 115 0.425715833902359 0.875\n",
      "1 120 0.38443800806999207 0.875\n",
      "1 125 0.3891093134880066 1.0\n",
      "1 130 0.321931928396225 1.0\n",
      "1 135 0.33903902769088745 1.0\n",
      "1 140 0.34518975019454956 1.0\n",
      "1 145 0.5578344464302063 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 150 0.5602446794509888 0.75\n",
      "1 155 0.3324597179889679 1.0\n",
      "1 160 0.3995357155799866 0.875\n",
      "1 165 0.333290159702301 1.0\n",
      "1 170 0.5937380194664001 0.75\n",
      "1 175 0.6031216382980347 0.75\n",
      "1 180 0.35814058780670166 1.0\n",
      "1 185 0.7196798324584961 0.5\n",
      "1 190 0.47112563252449036 0.875\n",
      "1 195 0.5544594526290894 0.75\n",
      "1 200 0.44024595618247986 0.875\n",
      "1 205 0.3181207776069641 1.0\n",
      "1 210 0.3274097740650177 1.0\n",
      "1 215 0.36965516209602356 1.0\n",
      "1 220 0.5725760459899902 0.75\n",
      "1 225 0.45976656675338745 0.875\n",
      "1 230 0.32977980375289917 1.0\n",
      "1 235 0.5687647461891174 0.75\n",
      "1 240 0.5978078842163086 0.75\n",
      "1 245 0.5180699229240417 0.875\n",
      "1 250 0.39836031198501587 0.875\n",
      "1 255 0.5301960110664368 0.75\n",
      "1 260 0.6325533390045166 0.75\n",
      "1 265 0.38303515315055847 1.0\n",
      "1 270 0.3301967680454254 1.0\n",
      "1 275 0.32930970191955566 1.0\n",
      "1 280 0.5607427358627319 0.75\n",
      "1 285 0.4778633117675781 0.875\n",
      "1 290 0.552074670791626 0.75\n",
      "1 295 0.5836372375488281 0.75\n",
      "1 300 0.6109071373939514 0.625\n",
      "1 305 0.32744449377059937 1.0\n",
      "1 310 0.49642616510391235 0.75\n",
      "1 315 0.47323015332221985 0.75\n",
      "1 320 0.3495224416255951 1.0\n",
      "1 325 0.5395376682281494 0.625\n",
      "1 330 0.5404587984085083 0.75\n",
      "1 335 0.44522345066070557 0.875\n",
      "1 340 0.37721487879753113 0.875\n",
      "1 345 0.708857536315918 0.625\n",
      "1 350 0.4881651997566223 0.75\n",
      "1 355 0.4503416419029236 0.75\n",
      "1 360 0.4548296332359314 0.875\n",
      "1 365 0.5310024619102478 0.875\n",
      "1 370 0.4584599435329437 0.875\n",
      "1 375 0.5758389234542847 0.75\n",
      "1 380 0.519116222858429 0.875\n",
      "1 385 0.5400635600090027 0.75\n",
      "1 390 0.5030487179756165 0.75\n",
      "1 395 0.3682437539100647 1.0\n",
      "1 400 0.3708369731903076 1.0\n",
      "1 405 0.4712461829185486 0.875\n",
      "1 410 0.5102652311325073 0.75\n",
      "1 415 0.6262691020965576 0.625\n",
      "1 420 0.4978356957435608 0.75\n",
      "1 425 0.3820011615753174 1.0\n",
      "1 430 0.42800796031951904 0.875\n",
      "1 435 0.468325138092041 0.875\n",
      "1 440 0.40713730454444885 1.0\n",
      "1 445 0.537650465965271 0.75\n",
      "1 450 0.5089176893234253 0.75\n",
      "1 455 0.4487573504447937 0.875\n",
      "1 460 0.5548564195632935 0.75\n",
      "1 465 0.38070914149284363 0.875\n",
      "1 470 0.47332897782325745 0.75\n",
      "1 475 0.4422902464866638 0.875\n",
      "1 480 0.46593791246414185 0.875\n",
      "1 485 0.43906131386756897 0.875\n",
      "1 490 0.3468688130378723 1.0\n",
      "1 495 0.6468682289123535 0.5\n",
      "1 500 0.45001083612442017 0.875\n",
      "1 505 0.5770658254623413 0.75\n",
      "1 510 0.451348215341568 0.75\n",
      "1 515 0.5197651386260986 0.75\n",
      "1 520 0.48445767164230347 0.875\n",
      "1 525 0.39463484287261963 0.875\n",
      "1 530 0.5860840678215027 0.75\n",
      "1 535 0.5550510883331299 0.75\n",
      "1 540 0.40423983335494995 0.875\n",
      "1 545 0.6038047671318054 0.625\n",
      "1 550 0.5625185966491699 0.75\n",
      "1 555 0.3841239809989929 1.0\n",
      "1 560 0.6617656946182251 0.625\n",
      "1 565 0.5541550517082214 0.75\n",
      "1 570 0.4434382915496826 0.875\n",
      "1 575 0.5323598384857178 0.75\n",
      "1 580 0.32934337854385376 1.0\n",
      "1 585 0.49599218368530273 0.75\n",
      "1 590 0.49187904596328735 0.75\n",
      "1 595 0.6836691498756409 0.625\n",
      "1 600 0.5082123279571533 0.75\n",
      "1 605 0.41859886050224304 0.875\n",
      "1 610 0.359790563583374 1.0\n",
      "1 615 0.34548312425613403 1.0\n",
      "1 620 0.35558366775512695 1.0\n",
      "1 625 0.9452809691429138 0.25\n",
      "1 630 0.33087849617004395 1.0\n",
      "1 635 0.48811906576156616 0.875\n",
      "1 640 0.3702283501625061 1.0\n",
      "1 645 0.4042280316352844 0.875\n",
      "1 650 0.4139374792575836 0.875\n",
      "1 655 0.44234195351600647 1.0\n",
      "1 660 0.4291064739227295 0.875\n",
      "1 665 0.4039381742477417 0.875\n",
      "1 670 0.510334312915802 0.75\n",
      "1 675 0.6966934204101562 0.625\n",
      "1 680 0.7456402778625488 0.5\n",
      "1 685 0.5692189335823059 0.75\n",
      "1 690 0.41188928484916687 0.875\n",
      "1 695 0.5854973196983337 0.75\n",
      "1 700 0.3530205488204956 1.0\n",
      "1 705 0.43229448795318604 0.875\n",
      "1 710 0.4820082187652588 0.75\n",
      "1 715 0.531041145324707 0.75\n",
      "1 720 0.4482652544975281 0.875\n",
      "1 725 0.6751958727836609 0.625\n",
      "1 730 0.3835790157318115 0.875\n",
      "1 735 0.37796422839164734 0.875\n",
      "1 740 0.33238714933395386 1.0\n",
      "1 745 0.5811848640441895 0.625\n",
      "1 750 0.512151837348938 0.75\n",
      "1 755 0.5841217637062073 0.75\n",
      "1 760 0.6939607262611389 0.5\n",
      "1 765 0.3350110650062561 1.0\n",
      "1 770 0.7095189094543457 0.5\n",
      "1 775 0.3797052800655365 1.0\n",
      "1 780 0.569586992263794 0.625\n",
      "1 785 0.46273553371429443 0.875\n",
      "1 790 0.3912203013896942 0.875\n",
      "1 795 0.3189619481563568 1.0\n",
      "1 800 0.3373870849609375 1.0\n",
      "1 805 0.4473141133785248 0.875\n",
      "1 810 0.3604762554168701 1.0\n",
      "1 815 0.7162527441978455 0.625\n",
      "1 820 0.5404789447784424 0.75\n",
      "1 825 0.6225777864456177 0.625\n",
      "1 830 0.5023409128189087 0.75\n",
      "1 835 0.43997669219970703 1.0\n",
      "1 840 0.5808489918708801 0.75\n",
      "1 845 0.7169965505599976 0.5\n",
      "1 850 0.4208866357803345 0.875\n",
      "1 855 0.4740552008152008 0.875\n",
      "1 860 0.5325895547866821 0.875\n",
      "1 865 0.5395113229751587 0.75\n",
      "1 870 0.3244357407093048 1.0\n",
      "1 875 0.44328826665878296 0.875\n",
      "1 880 0.3589306175708771 1.0\n",
      "1 885 0.3219231963157654 1.0\n",
      "1 890 0.34530526399612427 1.0\n",
      "1 895 0.6954158544540405 0.625\n",
      "1 900 0.7014232873916626 0.625\n",
      "1 905 0.6795255541801453 0.625\n",
      "1 910 0.3566721975803375 1.0\n",
      "1 915 0.32464879751205444 1.0\n",
      "1 920 0.32371363043785095 1.0\n",
      "1 925 0.42241352796554565 0.875\n",
      "1 930 0.6077555418014526 0.625\n",
      "1 935 0.47329089045524597 0.875\n",
      "1 940 0.5611916780471802 0.75\n",
      "1 945 0.40179166197776794 0.875\n",
      "1 950 0.3310215175151825 1.0\n",
      "1 955 0.47834062576293945 0.875\n",
      "1 960 0.5468703508377075 0.75\n",
      "1 965 0.4629713296890259 0.875\n",
      "1 970 0.5876656174659729 0.75\n",
      "1 975 0.45400166511535645 0.875\n",
      "1 980 0.5725247859954834 0.75\n",
      "1 985 0.514660120010376 0.75\n",
      "1 990 0.46149107813835144 0.875\n",
      "1 995 0.6599440574645996 0.625\n",
      "1 1000 0.4973243474960327 0.75\n",
      "1 1005 0.3402148485183716 1.0\n",
      "1 1010 0.4753132164478302 0.875\n",
      "1 1015 0.34617167711257935 1.0\n",
      "1 1020 0.5796610116958618 0.75\n",
      "1 1025 0.4882022440433502 0.875\n",
      "1 1030 0.6824063658714294 0.625\n",
      "1 1035 0.4550834000110626 0.875\n",
      "1 1040 0.6462134122848511 0.625\n",
      "1 1045 0.46089643239974976 0.875\n",
      "1 1050 0.3176555931568146 1.0\n",
      "1 1055 0.4728669822216034 0.875\n",
      "1 1060 0.4222988486289978 0.875\n",
      "1 1065 0.44405823945999146 0.875\n",
      "1 1070 0.5480700135231018 0.75\n",
      "1 1075 0.780707597732544 0.5\n",
      "1 1080 0.45550185441970825 0.875\n",
      "1 1085 0.470547616481781 0.875\n",
      "1 1090 0.3480026423931122 1.0\n",
      "1 1095 0.32505539059638977 1.0\n",
      "1 1100 0.4901743531227112 0.75\n",
      "1 1105 0.5048415064811707 0.625\n",
      "1 1110 0.43973851203918457 0.875\n",
      "1 1115 0.4727141559123993 0.875\n",
      "1 1120 0.3541732430458069 1.0\n",
      "1 1125 0.3657649755477905 1.0\n",
      "1 1130 0.4082314074039459 0.875\n",
      "1 1135 0.5502928495407104 0.625\n",
      "1 1140 0.41313278675079346 0.875\n",
      "1 1145 0.4860794246196747 0.75\n",
      "1 1150 0.48915600776672363 0.625\n",
      "1 1155 0.5398392081260681 0.75\n",
      "1 1160 0.5592899322509766 0.75\n",
      "1 1165 0.46990305185317993 0.875\n",
      "1 1170 0.44301101565361023 0.875\n",
      "1 1175 0.338898628950119 1.0\n",
      "1 1180 0.442651629447937 0.875\n",
      "1 1185 0.7730746865272522 0.5\n",
      "1 1190 0.5753516554832458 0.75\n",
      "1 1195 0.42288029193878174 0.875\n",
      "1 参数保存成功。\n",
      "2 0 0.3449569046497345 1.0\n",
      "2 5 0.423989474773407 0.875\n",
      "2 10 0.7275570034980774 0.5\n",
      "2 15 0.5258083343505859 0.875\n",
      "2 20 0.4034605324268341 0.875\n",
      "2 25 0.7288772463798523 0.5\n",
      "2 30 0.4515351355075836 0.875\n",
      "2 35 0.5451267957687378 0.75\n",
      "2 40 0.3884502351284027 0.875\n",
      "2 45 0.33114898204803467 1.0\n",
      "2 50 0.5561172366142273 0.75\n",
      "2 55 0.43731528520584106 0.875\n",
      "2 60 0.3699975907802582 1.0\n",
      "2 65 0.49893152713775635 0.875\n",
      "2 70 0.36905282735824585 0.875\n",
      "2 75 0.4280633330345154 0.875\n",
      "2 80 0.31996023654937744 1.0\n",
      "2 85 0.41263559460639954 0.875\n",
      "2 90 0.480635404586792 0.875\n",
      "2 95 0.5815376043319702 0.75\n",
      "2 100 0.5008972883224487 0.75\n",
      "2 105 0.5522464513778687 0.75\n",
      "2 110 0.3474443554878235 1.0\n",
      "2 115 0.6132611632347107 0.625\n",
      "2 120 0.4835173487663269 0.875\n",
      "2 125 0.5184363722801208 0.75\n",
      "2 130 0.3811809718608856 0.875\n",
      "2 135 0.3310657739639282 1.0\n",
      "2 140 0.32906922698020935 1.0\n",
      "2 145 0.3845328092575073 1.0\n",
      "2 150 0.42130059003829956 0.875\n",
      "2 155 0.45665210485458374 0.875\n",
      "2 160 0.5545307397842407 0.75\n",
      "2 165 0.32195600867271423 1.0\n",
      "2 170 0.32260555028915405 1.0\n",
      "2 175 0.4256311357021332 0.875\n",
      "2 180 0.3456924557685852 1.0\n",
      "2 185 0.4437165856361389 0.875\n",
      "2 190 0.48908746242523193 0.875\n",
      "2 195 0.43764546513557434 0.875\n",
      "2 200 0.41574159264564514 0.875\n",
      "2 205 0.486121267080307 0.75\n",
      "2 210 0.7969961166381836 0.5\n",
      "2 215 0.40189531445503235 0.875\n",
      "2 220 0.40383201837539673 0.875\n",
      "2 225 0.33242952823638916 1.0\n",
      "2 230 0.682957649230957 0.625\n",
      "2 235 0.47014710307121277 0.875\n",
      "2 240 0.37311676144599915 0.875\n",
      "2 245 0.5149261355400085 0.75\n",
      "2 250 0.3563638925552368 1.0\n",
      "2 255 0.47213608026504517 0.875\n",
      "2 260 0.5056387782096863 0.75\n",
      "2 265 0.3963862955570221 0.875\n",
      "2 270 0.6079291105270386 0.625\n",
      "2 275 0.7190967798233032 0.625\n",
      "2 280 0.4544317126274109 0.875\n",
      "2 285 0.3140636384487152 1.0\n",
      "2 290 0.44180285930633545 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 295 0.6369665861129761 0.625\n",
      "2 300 0.6557157039642334 0.625\n",
      "2 305 0.5747004747390747 0.75\n",
      "2 310 0.3277896046638489 1.0\n",
      "2 315 0.3732028007507324 1.0\n",
      "2 320 0.5769637227058411 0.75\n",
      "2 325 0.42758625745773315 0.875\n",
      "2 330 0.3167629837989807 1.0\n",
      "2 335 0.44729095697402954 0.875\n",
      "2 340 0.7451139092445374 0.5\n",
      "2 345 0.5591530203819275 0.75\n",
      "2 350 0.403085321187973 0.875\n",
      "2 355 0.5934910178184509 0.625\n",
      "2 360 0.45056262612342834 0.875\n",
      "2 365 0.31672534346580505 1.0\n",
      "2 370 0.31622231006622314 1.0\n",
      "2 375 0.5475991368293762 0.75\n",
      "2 380 0.3265668749809265 1.0\n",
      "2 385 0.4365466237068176 0.875\n",
      "2 390 0.3320382833480835 1.0\n",
      "2 395 0.3158762454986572 1.0\n",
      "2 400 0.42778947949409485 0.875\n",
      "2 405 0.5590391755104065 0.75\n",
      "2 410 0.5603075623512268 0.75\n",
      "2 415 0.5021701455116272 0.75\n",
      "2 420 0.5637964010238647 0.75\n",
      "2 425 0.5012670755386353 0.75\n",
      "2 430 0.4592035412788391 0.75\n",
      "2 435 0.3222958743572235 1.0\n",
      "2 440 0.5266078114509583 0.75\n",
      "2 445 0.31861138343811035 1.0\n",
      "2 450 0.554032564163208 0.625\n",
      "2 455 0.3153887391090393 1.0\n",
      "2 460 0.5409671068191528 0.75\n",
      "2 465 0.5256081819534302 0.875\n",
      "2 470 0.37331628799438477 0.875\n",
      "2 475 0.33122313022613525 1.0\n",
      "2 480 0.6592474579811096 0.625\n",
      "2 485 0.3662981390953064 1.0\n",
      "2 490 0.43377918004989624 0.875\n",
      "2 495 0.6110742092132568 0.75\n",
      "2 500 0.5923283100128174 0.625\n",
      "2 505 0.5004196166992188 0.75\n",
      "2 510 0.7207691073417664 0.5\n",
      "2 515 0.32535111904144287 1.0\n",
      "2 520 0.47310730814933777 0.875\n",
      "2 525 0.37911859154701233 0.875\n",
      "2 530 0.5100960731506348 0.875\n",
      "2 535 0.387855589389801 1.0\n",
      "2 540 0.7526358366012573 0.5\n",
      "2 545 0.330386221408844 1.0\n",
      "2 550 0.4516748785972595 0.875\n",
      "2 555 0.4676244556903839 0.875\n",
      "2 560 0.4690752625465393 0.875\n",
      "2 565 0.40397512912750244 1.0\n",
      "2 570 0.530960738658905 0.75\n",
      "2 575 0.6152300834655762 0.625\n",
      "2 580 0.35979142785072327 1.0\n",
      "2 585 0.5099640488624573 0.75\n",
      "2 590 0.318028062582016 1.0\n",
      "2 595 0.3332703709602356 1.0\n",
      "2 600 0.4194086790084839 0.875\n",
      "2 605 0.3389483392238617 1.0\n",
      "2 610 0.5014761686325073 0.875\n",
      "2 615 0.37652498483657837 0.875\n",
      "2 620 0.35068896412849426 1.0\n",
      "2 625 0.35532259941101074 1.0\n",
      "2 630 0.42759960889816284 0.875\n",
      "2 635 0.44611990451812744 0.875\n",
      "2 640 0.348055899143219 1.0\n",
      "2 645 0.7448955774307251 0.625\n",
      "2 650 0.6602800488471985 0.625\n",
      "2 655 0.44996681809425354 0.875\n",
      "2 660 0.3815607726573944 1.0\n",
      "2 665 0.34240269660949707 1.0\n",
      "2 670 0.5109009146690369 0.75\n",
      "2 675 0.41222602128982544 0.875\n",
      "2 680 0.4071745276451111 0.875\n",
      "2 685 0.3249223232269287 1.0\n",
      "2 690 0.3216121196746826 1.0\n",
      "2 695 0.5636675357818604 0.75\n",
      "2 700 0.6878060102462769 0.5\n",
      "2 705 0.3662612736225128 1.0\n",
      "2 710 0.40433308482170105 0.875\n",
      "2 715 0.40314406156539917 0.875\n",
      "2 720 0.3964169919490814 0.875\n",
      "2 725 0.4953150749206543 0.75\n",
      "2 730 0.3616795241832733 1.0\n",
      "2 735 0.49285057187080383 0.75\n",
      "2 740 0.46063923835754395 0.875\n",
      "2 745 0.5555323958396912 0.75\n",
      "2 750 0.4105105400085449 0.875\n",
      "2 755 0.31587767601013184 1.0\n",
      "2 760 0.34592148661613464 1.0\n",
      "2 765 0.42345547676086426 0.875\n",
      "2 770 0.35159868001937866 1.0\n",
      "2 775 0.5266079306602478 0.75\n",
      "2 780 0.6673277020454407 0.625\n",
      "2 785 0.5287764072418213 0.75\n",
      "2 790 0.7112417221069336 0.5\n",
      "2 795 0.4261398911476135 0.875\n",
      "2 800 0.4584646224975586 0.875\n",
      "2 805 0.4720340967178345 0.75\n",
      "2 810 0.32591190934181213 1.0\n",
      "2 815 0.31566858291625977 1.0\n",
      "2 820 0.5123327970504761 0.75\n",
      "2 825 0.6616737842559814 0.625\n",
      "2 830 0.3189452290534973 1.0\n",
      "2 835 0.5480886697769165 0.75\n",
      "2 840 0.42637762427330017 0.875\n",
      "2 845 0.33284682035446167 1.0\n",
      "2 850 0.4486888349056244 0.875\n",
      "2 855 0.3202107548713684 1.0\n",
      "2 860 0.534079372882843 0.875\n",
      "2 865 0.3942994177341461 0.875\n",
      "2 870 0.6051565408706665 0.625\n",
      "2 875 0.34425607323646545 1.0\n",
      "2 880 0.4340006411075592 0.875\n",
      "2 885 0.4447515904903412 0.875\n",
      "2 890 0.4495050609111786 0.875\n",
      "2 895 0.4550473093986511 0.75\n",
      "2 900 0.5975311994552612 0.75\n",
      "2 905 0.3584387004375458 1.0\n",
      "2 910 0.5079944133758545 0.875\n",
      "2 915 0.4838912785053253 0.75\n",
      "2 920 0.5194538235664368 0.75\n",
      "2 925 0.4410204589366913 0.875\n",
      "2 930 0.6093730926513672 0.75\n",
      "2 935 0.6062514781951904 0.75\n",
      "2 940 0.3224705457687378 1.0\n",
      "2 945 0.55173259973526 0.75\n",
      "2 950 0.6151266098022461 0.625\n",
      "2 955 0.328761488199234 1.0\n",
      "2 960 0.5452754497528076 0.75\n",
      "2 965 0.43733757734298706 0.875\n",
      "2 970 0.5393328666687012 0.75\n",
      "2 975 0.4327690303325653 1.0\n",
      "2 980 0.5276885628700256 0.625\n",
      "2 985 0.44825178384780884 0.875\n",
      "2 990 0.4381771683692932 0.875\n",
      "2 995 0.3225768208503723 1.0\n",
      "2 1000 0.5686323642730713 0.75\n",
      "2 1005 0.4803059995174408 0.875\n",
      "2 1010 0.3408985733985901 1.0\n",
      "2 1015 0.57249915599823 0.625\n",
      "2 1020 0.5871513485908508 0.75\n",
      "2 1025 0.34924837946891785 1.0\n",
      "2 1030 0.3175840675830841 1.0\n",
      "2 1035 0.38118159770965576 0.875\n",
      "2 1040 0.48157066106796265 0.875\n",
      "2 1045 0.852624773979187 0.375\n",
      "2 1050 0.4541008472442627 0.875\n",
      "2 1055 0.44182392954826355 0.875\n",
      "2 1060 0.34749943017959595 1.0\n",
      "2 1065 0.3180423974990845 1.0\n",
      "2 1070 0.322093665599823 1.0\n",
      "2 1075 0.40461570024490356 0.875\n",
      "2 1080 0.4593327045440674 0.875\n",
      "2 1085 0.4770810306072235 0.875\n",
      "2 1090 0.3311363458633423 1.0\n",
      "2 1095 0.48486801981925964 0.875\n",
      "2 1100 0.46420377492904663 0.875\n",
      "2 1105 0.49956372380256653 0.75\n",
      "2 1110 0.3255743682384491 1.0\n",
      "2 1115 0.3942732810974121 0.875\n",
      "2 1120 0.40677475929260254 1.0\n",
      "2 1125 0.6259356141090393 0.75\n",
      "2 1130 0.5786474943161011 0.75\n",
      "2 1135 0.527968168258667 0.75\n",
      "2 1140 0.6335557699203491 0.75\n",
      "2 1145 0.38130778074264526 0.875\n",
      "2 1150 0.6038777828216553 0.625\n",
      "2 1155 0.4623669981956482 0.875\n",
      "2 1160 0.4509196877479553 0.875\n",
      "2 1165 0.44792503118515015 0.875\n",
      "2 1170 0.3820112347602844 0.875\n",
      "2 1175 0.5738767981529236 0.75\n",
      "2 1180 0.5160244703292847 0.625\n",
      "2 1185 0.37705710530281067 1.0\n",
      "2 1190 0.38863831758499146 0.875\n",
      "2 1195 0.3154262602329254 1.0\n",
      "2 参数保存成功。\n",
      "3 0 0.46481943130493164 0.875\n",
      "3 5 0.3385211229324341 1.0\n",
      "3 10 0.44026198983192444 0.875\n",
      "3 15 0.546829104423523 0.75\n",
      "3 20 0.44367289543151855 0.875\n",
      "3 25 0.3541797697544098 1.0\n",
      "3 30 0.3382640480995178 1.0\n",
      "3 35 0.32883158326148987 1.0\n",
      "3 40 0.4305305480957031 0.875\n",
      "3 45 0.43330278992652893 0.875\n",
      "3 50 0.44611549377441406 0.875\n",
      "3 55 0.3482840657234192 1.0\n",
      "3 60 0.3404814600944519 1.0\n",
      "3 65 0.38648876547813416 0.875\n",
      "3 70 0.5949851274490356 0.625\n",
      "3 75 0.3987560272216797 0.875\n",
      "3 80 0.3185110092163086 1.0\n",
      "3 85 0.4424959123134613 0.875\n",
      "3 90 0.5662092566490173 0.75\n",
      "3 95 0.6962525248527527 0.625\n",
      "3 100 0.3951954245567322 0.875\n",
      "3 105 0.43650758266448975 0.875\n",
      "3 110 0.6266235113143921 0.625\n",
      "3 115 0.42593955993652344 1.0\n",
      "3 120 0.5152361989021301 0.75\n",
      "3 125 0.3600805699825287 1.0\n",
      "3 130 0.37684449553489685 1.0\n",
      "3 135 0.5222294330596924 0.75\n",
      "3 140 0.49145376682281494 0.75\n",
      "3 145 0.31592461466789246 1.0\n",
      "3 150 0.47951406240463257 0.875\n",
      "3 155 0.4506233334541321 0.875\n",
      "3 160 0.4940682351589203 0.875\n",
      "3 165 0.47767552733421326 0.875\n",
      "3 170 0.4942835867404938 0.875\n",
      "3 175 0.5151187777519226 0.875\n",
      "3 180 0.5081525444984436 0.75\n",
      "3 185 0.3402007818222046 1.0\n",
      "3 190 0.5034507513046265 0.75\n",
      "3 195 0.31499144434928894 1.0\n",
      "3 200 0.3196227550506592 1.0\n",
      "3 205 0.412284255027771 0.875\n",
      "3 210 0.373149573802948 1.0\n",
      "3 215 0.4115717113018036 0.875\n",
      "3 220 0.47548648715019226 0.875\n",
      "3 225 0.6106210947036743 0.75\n",
      "3 230 0.4849969148635864 0.75\n",
      "3 235 0.47432222962379456 0.75\n",
      "3 240 0.6632660031318665 0.625\n",
      "3 245 0.5149298906326294 0.875\n",
      "3 250 0.4573918581008911 0.75\n",
      "3 255 0.31793612241744995 1.0\n",
      "3 260 0.43948638439178467 0.875\n",
      "3 265 0.34771469235420227 1.0\n",
      "3 270 0.3236193060874939 1.0\n",
      "3 275 0.3880922794342041 1.0\n",
      "3 280 0.3153317868709564 1.0\n",
      "3 285 0.36435481905937195 1.0\n",
      "3 290 0.540160059928894 0.75\n",
      "3 295 0.45940420031547546 0.875\n",
      "3 300 0.5571354627609253 0.625\n",
      "3 305 0.44804733991622925 0.875\n",
      "3 310 0.43661192059516907 0.875\n",
      "3 315 0.3321385979652405 1.0\n",
      "3 320 0.3871104419231415 0.875\n",
      "3 325 0.4903346002101898 0.875\n",
      "3 330 0.6622544527053833 0.625\n",
      "3 335 0.4923107326030731 0.75\n",
      "3 340 0.47496041655540466 0.875\n",
      "3 345 0.5146104693412781 0.75\n",
      "3 350 0.4668743312358856 0.875\n",
      "3 355 0.3588010370731354 1.0\n",
      "3 360 0.3561631441116333 1.0\n",
      "3 365 0.37956735491752625 1.0\n",
      "3 370 0.47715267539024353 0.875\n",
      "3 375 0.4402691125869751 0.875\n",
      "3 380 0.5619208812713623 0.75\n",
      "3 385 0.5764102339744568 0.625\n",
      "3 390 0.4322471618652344 0.875\n",
      "3 395 0.5654105544090271 0.75\n",
      "3 400 0.33334988355636597 1.0\n",
      "3 405 0.3277508020401001 1.0\n",
      "3 410 0.4432484209537506 0.875\n",
      "3 415 0.5689390301704407 0.75\n",
      "3 420 0.43982189893722534 0.875\n",
      "3 425 0.490276038646698 0.875\n",
      "3 430 0.3362167179584503 1.0\n",
      "3 435 0.5015678405761719 0.75\n",
      "3 440 0.4166194498538971 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 445 0.7103617787361145 0.625\n",
      "3 450 0.6780800223350525 0.5\n",
      "3 455 0.4601452052593231 0.75\n",
      "3 460 0.44828835129737854 0.875\n",
      "3 465 0.40021395683288574 0.875\n",
      "3 470 0.3759569525718689 0.875\n",
      "3 475 0.47413158416748047 0.875\n",
      "3 480 0.6647790670394897 0.625\n",
      "3 485 0.5549802184104919 0.75\n",
      "3 490 0.32065549492836 1.0\n",
      "3 495 0.5366483926773071 0.75\n",
      "3 500 0.446577787399292 0.875\n",
      "3 505 0.36841392517089844 0.875\n",
      "3 510 0.45536044239997864 0.875\n",
      "3 515 0.4627752900123596 0.875\n",
      "3 520 0.4692034125328064 0.75\n",
      "3 525 0.36862483620643616 1.0\n",
      "3 530 0.4176875948905945 0.875\n",
      "3 535 0.6634545922279358 0.625\n",
      "3 540 0.4542435109615326 0.875\n",
      "3 545 0.3270670473575592 1.0\n",
      "3 550 0.5982930660247803 0.75\n",
      "3 555 0.6666509509086609 0.5\n",
      "3 560 0.34704938530921936 1.0\n",
      "3 565 0.5633231401443481 0.75\n",
      "3 570 0.7127703428268433 0.5\n",
      "3 575 0.5615665912628174 0.75\n",
      "3 580 0.6152815818786621 0.75\n",
      "3 585 0.39018723368644714 0.875\n",
      "3 590 0.5664861798286438 0.75\n",
      "3 595 0.3157813847064972 1.0\n",
      "3 600 0.3377455770969391 1.0\n",
      "3 605 0.4556068480014801 0.875\n",
      "3 610 0.6285353302955627 0.5\n",
      "3 615 0.4131907820701599 0.875\n",
      "3 620 0.53780198097229 0.75\n",
      "3 625 0.5093745589256287 0.75\n",
      "3 630 0.7568473815917969 0.5\n",
      "3 635 0.32707449793815613 1.0\n",
      "3 640 0.5808793902397156 0.75\n",
      "3 645 0.38089719414711 1.0\n",
      "3 650 0.5803149938583374 0.75\n",
      "3 655 0.5871587991714478 0.625\n",
      "3 660 0.6794463992118835 0.625\n",
      "3 665 0.6686593890190125 0.625\n",
      "3 670 0.4734979271888733 0.875\n",
      "3 675 0.3796162009239197 0.875\n",
      "3 680 0.3639916181564331 0.875\n",
      "3 685 0.33127835392951965 1.0\n",
      "3 690 0.4451438784599304 0.875\n",
      "3 695 0.3803207576274872 1.0\n",
      "3 700 0.584346354007721 0.625\n",
      "3 705 0.5215309262275696 0.75\n",
      "3 710 0.43663325905799866 0.875\n",
      "3 715 0.4745562672615051 0.875\n",
      "3 720 0.7114558219909668 0.625\n",
      "3 725 0.33754491806030273 1.0\n",
      "3 730 0.4310394823551178 0.875\n",
      "3 735 0.44336774945259094 0.875\n",
      "3 740 0.4878461956977844 0.875\n",
      "3 745 0.4516354203224182 0.875\n",
      "3 750 0.9033223986625671 0.375\n",
      "3 755 0.6419389843940735 0.625\n",
      "3 760 0.3607952892780304 1.0\n",
      "3 765 0.48924508690834045 0.875\n",
      "3 770 0.519605278968811 0.75\n",
      "3 775 0.5339868664741516 0.75\n",
      "3 780 0.6580841541290283 0.5\n",
      "3 785 0.37019795179367065 0.875\n",
      "3 790 0.3190125823020935 1.0\n",
      "3 795 0.3216363787651062 1.0\n",
      "3 800 0.5613505244255066 0.75\n",
      "3 805 0.5650672912597656 0.75\n",
      "3 810 0.32318055629730225 1.0\n",
      "3 815 0.5519806146621704 0.75\n",
      "3 820 0.3928048610687256 0.875\n",
      "3 825 0.45902779698371887 0.875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m input_ids,attention_mask,token_type_ids,labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(DEVICE),attention_mask\u001b[38;5;241m.\u001b[39mto(DEVICE),token_type_ids\u001b[38;5;241m.\u001b[39mto(DEVICE),labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 执行前向计算\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m out\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out,labels)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 深度学习优化模型三步走\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m#1、清空权重梯度,2、反向传播，3、更新梯度\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,input_ids,attention_mask,token_type_ids):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mpretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     out\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     11\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\transformers\\pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:554\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Program Files\\miniconda\\lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2892\u001b[0m         layer_norm,\n\u001b[0;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2899\u001b[0m     )\n\u001b[1;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "from transformers import BertTokenizer,AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "EPOCH=10\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 对数据进行编码处理\n",
    "def collate_fn(data):\n",
    "    sentes = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "    data = tokenizer.batch_encode_plus(\n",
    "        batch_text_or_text_pairs=sentes,\n",
    "        padding=\"max_length\",  # 填充到最大长度\n",
    "        truncation=True,       # 截断超过最大长度的序列\n",
    "        max_length=30,         # 最大长度为 10\n",
    "        return_length=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = data[\"input_ids\"]\n",
    "    attention_mask = data[\"attention_mask\"]\n",
    "    token_type_ids = data[\"token_type_ids\"]\n",
    "    labels = torch.LongTensor(labels)\n",
    "    \n",
    "    return input_ids,attention_mask,token_type_ids,labels\n",
    "\n",
    "# 1、创建数据集\n",
    "train_ds= BizDataset(\"train\")\n",
    "# 2、创建数据加载器\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "#训练\n",
    "if __name__=='__main__':\n",
    "    print(DEVICE)\n",
    "    model=Model().to(DEVICE)\n",
    "    optimizer=AdamW(model.parameters(),lr=1e-3)    \n",
    "    loss_func =torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(EPOCH):\n",
    "        for i ,(input_ids,attention_mask,token_type_ids,labels) in enumerate(train_loader):\n",
    "            input_ids,attention_mask,token_type_ids,labels = input_ids.to(DEVICE),attention_mask.to(DEVICE),token_type_ids.to(DEVICE),labels.to(DEVICE)\n",
    "            # 执行前向计算\n",
    "            out= model(input_ids,attention_mask,token_type_ids)\n",
    "            loss = loss_func(out,labels)\n",
    "            \n",
    "            # 深度学习优化模型三步走\n",
    "            #1、清空权重梯度,2、反向传播，3、更新梯度\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i%5 == 0:\n",
    "                out = out.argmax(dim=1)\n",
    "                acc = (out == labels).sum().item()/len(labels)\n",
    "                print(epoch,i,loss.item(),acc)\n",
    "        \n",
    "        # 保存模型参数\n",
    "        torch.save(model.state_dict(),f\"params/{epoch}bert.pt\")\n",
    "        print(epoch,\"参数保存成功。\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f338d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
