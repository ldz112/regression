{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7347df2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0000, -0.7778, -0.5556, -0.3333, -0.1111,  0.1111,  0.3333,  0.5556,\n",
       "          0.7778,  1.0000]),\n",
       " tensor([-0.7616, -0.6514, -0.5047, -0.3215, -0.1107,  0.1107,  0.3215,  0.5047,\n",
       "          0.6514,  0.7616]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "z= torch.linspace(-1,1,10)\n",
    "a= torch.tanh(z)\n",
    "z,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f666e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1111, 0.3333, 0.5556, 0.7778,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=torch.relu(z)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dcb1c",
   "metadata": {},
   "source": [
    "# MSE(mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "841a1bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# -*-\n",
    "__author__ = 'dz.lee'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x= torch.ones(1)\n",
    "w= torch.tensor([2.],requires_grad=True)\n",
    "#w= torch.full([1]，2.)\n",
    "#如果在定义w的时候没有说明需要计算梯度，则使用下面语句指定\n",
    "# w.requires grad()\n",
    "mse =F.mse_loss(torch.ones(1),x*w)\n",
    "#梯度计算方法1,假设里面又N个w需要求导，\n",
    "# 则参数列表[w1,w2,w3,...,wn,b]，返回的是[dw1,dw2,dw3,...,dwn,db]\n",
    "# dw = torch.autograd.grad(mse，[w])\n",
    "# print(dw)\n",
    "# 梯度计算方法2，计算出w的梯度会直接赋给w1.grad,w2.grad,...,wn.grad,b.grad\n",
    "mse.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7531d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 2., 1.], requires_grad=True)\n",
      "tensor([0.6652, 0.2447, 0.0900], grad_fn=<SoftmaxBackward0>)\n",
      "tensor(0.4076, grad_fn=<NegBackward0>)\n",
      "tensor([-0.3348,  0.2447,  0.0900])\n"
     ]
    }
   ],
   "source": [
    "# softmax\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "#L层计算结果z\n",
    "z=torch.tensor([3.,2.,1.],requires_grad=True)\n",
    "print(z)\n",
    "#z经过softmax激活函数\n",
    "y_hat = F.softmax(z,dim=0)\n",
    "print(y_hat)\n",
    "\n",
    "# 假设标签是y\n",
    "y=torch.tensor([1.,0.,0.])\n",
    "#损失值1oss为交叉熵函数结果,dkl越小重合度越高\n",
    "# 为什么不使用MSE？\n",
    "# 1、sigmoid + MSE 的模式会导致梯度离散的现象\n",
    "# 2、收敛速度比较慢\n",
    "\n",
    "loss = -torch.sum(y * torch.log(y_hat))\n",
    "print(loss)\n",
    "#方法一\n",
    "loss.backward()\n",
    "print(z.grad)\n",
    "#方法二\n",
    "# dz = torch.autograd.grad(loss,[z],retain graph=True)\n",
    "# print(dz)#tensor([-0.3348,0.2447, 0.0900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30df60d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0:x=-0.0009999999310821295,f(x)=130.0\n",
      "step 2000:x=-2.237088918685913,f(x)=8.546550750732422\n",
      "step 4000:x=-2.803907632827759,f(x)=4.81572060380131e-05\n",
      "step 6000:x=-2.8051092624664307,f(x)=4.217326932121068e-09\n",
      "step 8000:x=-2.805114984512329,f(x)=5.32054400537163e-10\n",
      "step 10000:x=-2.805117130279541,f(x)=6.298250809777528e-11\n",
      "step 12000:x=-2.8051178455352783,f(x)=3.637978807091713e-12\n",
      "step 14000:x=-2.8051180839538574,f(x)=2.2737367544323206e-13\n",
      "step 16000:x=-2.8051180839538574,f(x)=2.2737367544323206e-13\n",
      "step 18000:x=-2.8051180839538574,f(x)=2.2737367544323206e-13\n"
     ]
    }
   ],
   "source": [
    "# 初步使用梯度下降优化参数 与人善言，暖于布帛；伤人以言，深于矛戟\n",
    "# 如果用到深度学习中，我们优化的目标函数为loss函数，优化对象为w和b(如果使用BN等，则还要优化vB等)\n",
    "import torch\n",
    "#当初始化改变时，通过梯度下降找到的局部最优解也时不同的\n",
    "#初始化x=4,y=4时，局部最优x=3.0,y=2.0\n",
    "#初始化x=0,y=4时，局部最优x=-2.8051,y=3.1313\n",
    "x = torch.tensor(0.,requires_grad=True)\n",
    "y = torch.tensor(4.,requires_grad=True)\n",
    "\n",
    "# x=torch.tensor([0.,0.],requires_grad=True)\n",
    "\n",
    "# def func(x):\n",
    "#    return (x[0]**2 + x[1]-11) **2 + (x[0] + x[1]**2-7) **2\n",
    "\n",
    "def func(x,y):\n",
    "   return (x**2 + y-11) **2 + (x + y**2-7) **2\n",
    "\n",
    "# loss\n",
    "optimizer =torch.optim.Adam([x,y],lr=1e-3)\n",
    "for step in range(20000):\n",
    "    pred = func(x,y)\n",
    "    optimizer.zero_grad()\n",
    "    pred.backward( )\n",
    "    optimizer.step()\n",
    "    if step % 2000 == 0:\n",
    "        print('step {}:x={},f(x)={}'\n",
    "         .format(step,x.tolist(),pred.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23f25342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "step: tensor(45.6821, grad_fn=<NllLossBackward0>)\n",
      "tensor([[1.8586e-24, 2.0352e-26, 3.3905e-17, 3.8623e-23, 1.0000e+00, 1.0888e-28,\n",
      "         1.7595e-31, 1.1532e-12, 1.1601e-07, 1.1408e-16],\n",
      "        [4.2887e-11, 3.8039e-21, 1.6906e-22, 2.2817e-04, 1.0405e-21, 1.3819e-05,\n",
      "         9.9976e-01, 6.9051e-25, 1.9721e-14, 3.3848e-13],\n",
      "        [1.0709e-11, 8.0543e-09, 2.5405e-21, 2.5435e-11, 2.9016e-25, 2.0394e-18,\n",
      "         4.9406e-18, 1.1520e-41, 1.0000e+00, 1.9023e-33],\n",
      "        [0.0000e+00, 5.4190e-03, 5.8946e-32, 9.5053e-38, 9.6325e-01, 1.0310e-24,\n",
      "         2.2432e-13, 4.3903e-42, 3.1334e-02, 5.1498e-25],\n",
      "        [4.4266e-18, 2.7544e-13, 2.5871e-10, 5.5524e-06, 1.0292e-04, 4.5565e-17,\n",
      "         1.3607e-29, 9.9988e-01, 3.5745e-30, 1.4104e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(7.1689e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor([[3.1289e-16, 9.9990e-01, 1.9616e-08, 2.2176e-14, 6.6279e-05, 2.7731e-20,\n",
      "         4.1367e-23, 7.4917e-06, 2.1914e-05, 3.3182e-10],\n",
      "        [3.1872e-11, 1.3576e-12, 1.6497e-10, 1.0966e-08, 9.9990e-01, 1.9451e-10,\n",
      "         7.1875e-05, 1.8926e-14, 2.8485e-05, 2.5380e-10],\n",
      "        [3.7238e-11, 4.2827e-05, 1.2617e-13, 8.8246e-09, 7.7517e-18, 4.0293e-11,\n",
      "         9.9991e-01, 2.1522e-34, 4.7094e-05, 4.7121e-26],\n",
      "        [0.0000e+00, 7.1276e-06, 9.9997e-01, 5.6254e-33, 1.7199e-05, 1.3708e-20,\n",
      "         1.3145e-08, 7.3507e-36, 5.3595e-06, 1.6257e-19],\n",
      "        [7.7137e-15, 3.5383e-11, 5.6198e-08, 1.5671e-08, 9.9996e-01, 3.0746e-14,\n",
      "         7.7473e-27, 4.2672e-05, 7.3835e-27, 7.7381e-10]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(2.0003e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor([[1.3332e-16, 9.9998e-01, 8.3667e-09, 1.0338e-14, 1.7806e-05, 1.1861e-20,\n",
      "         1.7659e-23, 1.5276e-06, 5.6324e-06, 7.4365e-11],\n",
      "        [1.0777e-11, 5.9799e-13, 7.7682e-11, 3.0025e-09, 9.9997e-01, 6.0751e-11,\n",
      "         1.9609e-05, 9.4485e-15, 5.9292e-06, 7.8142e-11],\n",
      "        [1.1375e-11, 1.0224e-05, 5.5265e-14, 2.2539e-09, 3.3808e-18, 1.4470e-11,\n",
      "         9.9998e-01, 9.3034e-35, 1.1338e-05, 2.0596e-26],\n",
      "        [0.0000e+00, 4.7637e-06, 9.9998e-01, 3.4415e-33, 1.0483e-05, 8.1422e-21,\n",
      "         7.9560e-09, 5.3592e-36, 2.8980e-06, 9.7260e-20],\n",
      "        [5.2282e-15, 2.2763e-11, 3.7168e-08, 3.6716e-09, 9.9999e-01, 2.0693e-14,\n",
      "         5.1818e-27, 9.9082e-06, 5.3014e-27, 1.9041e-10]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(7.0095e-06, grad_fn=<NllLossBackward0>)\n",
      "tensor([[6.6035e-17, 9.9999e-01, 4.2364e-09, 5.5504e-15, 6.0696e-06, 5.9361e-21,\n",
      "         8.8421e-24, 4.5428e-07, 1.8509e-06, 2.5618e-11],\n",
      "        [4.9278e-12, 3.1155e-13, 4.4094e-11, 1.0365e-09, 9.9999e-01, 2.4351e-11,\n",
      "         6.6658e-06, 5.4784e-15, 1.8147e-06, 3.0589e-11],\n",
      "        [4.9714e-12, 3.3085e-06, 2.9010e-14, 7.5597e-10, 1.7563e-18, 6.5987e-12,\n",
      "         9.9999e-01, 4.8136e-35, 3.6983e-06, 1.0768e-26],\n",
      "        [0.0000e+00, 2.2397e-06, 9.9999e-01, 1.7613e-33, 4.3714e-06, 4.1029e-21,\n",
      "         4.0276e-09, 3.1416e-36, 1.2086e-06, 4.9375e-20],\n",
      "        [3.9019e-15, 1.6262e-11, 2.7002e-08, 1.1979e-09, 1.0000e+00, 1.5448e-14,\n",
      "         3.8436e-27, 3.1890e-06, 4.1612e-27, 6.9164e-11]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(2.4795e-06, grad_fn=<NllLossBackward0>)\n",
      "tensor([[3.4053e-17, 1.0000e+00, 2.2563e-09, 3.0970e-15, 2.1969e-06, 3.0951e-21,\n",
      "         4.6339e-24, 1.5166e-07, 6.5309e-07, 1.0588e-11],\n",
      "        [2.5369e-12, 1.6941e-13, 2.6349e-11, 3.8010e-10, 1.0000e+00, 1.0910e-11,\n",
      "         2.4040e-06, 3.3073e-15, 6.1907e-07, 1.3476e-11],\n",
      "        [2.4844e-12, 1.1693e-06, 1.6054e-14, 2.7534e-10, 9.5981e-19, 3.3049e-12,\n",
      "         1.0000e+00, 2.6253e-35, 1.3124e-06, 5.9337e-27],\n",
      "        [0.0000e+00, 9.0873e-07, 1.0000e+00, 8.4212e-34, 1.6205e-06, 1.9483e-21,\n",
      "         1.9017e-09, 1.6961e-36, 4.5695e-07, 2.3588e-20],\n",
      "        [2.9770e-15, 1.1943e-11, 1.9981e-08, 4.3153e-10, 1.0000e+00, 1.1851e-14,\n",
      "         2.9328e-27, 1.1210e-06, 3.3374e-27, 3.0439e-11]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# cross entorpy 实现分类\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x= torch.randn(5,784)\n",
    "w= torch.randn(10,784,requires_grad=True) \n",
    "#L层计算结果z\n",
    "z =x @ w.t()\n",
    "z.requires_grad_()\n",
    "print(z.size())\n",
    "#注意，这里没有使用one-hot编码\n",
    "y=torch.tensor([1,4,6,2,4])\n",
    "# Adam优化器\n",
    "optimizer =torch.optim.Adam([w],lr=1e-3)\n",
    "for step in range(10000):\n",
    "    #每次更新w，重新计算z\n",
    "    z = x @ w.t()\n",
    "    optimizer.zero_grad()\n",
    "    # 使用cross entropy，z为输入[B,C]，B为batch size,c为classes。y输入标签，但不是one-hot\n",
    "    loss =F.cross_entropy(z,y)\n",
    "    # 反向传播\n",
    "    loss.backward(retain_graph=True)\n",
    "    #优化一次\n",
    "    optimizer.step()\n",
    "    if step % 2000 == 0:\n",
    "        print(\"step:\",loss)\n",
    "        #看看结果是否和标签一致\n",
    "        print(torch.softmax(z,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31d08ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n",
      "step: tensor(64.7711, grad_fn=<NllLossBackward0>)\n",
      "tensor([[3.5398e-35, 0.0000e+00, 2.4696e-23, 5.0736e-22, 1.0000e+00, 2.6199e-13,\n",
      "         3.6930e-29, 0.0000e+00, 7.3306e-27, 9.6540e-41],\n",
      "        [3.7861e-12, 3.3767e-17, 4.3553e-28, 2.2021e-04, 1.2101e-12, 3.7545e-10,\n",
      "         2.0702e-35, 3.5007e-24, 2.0122e-23, 9.9978e-01],\n",
      "        [6.3175e-04, 2.9766e-08, 1.3718e-15, 1.0107e-06, 9.3149e-22, 5.7225e-16,\n",
      "         1.2662e-33, 9.8001e-01, 1.9361e-02, 7.6777e-37],\n",
      "        [1.0000e+00, 2.8954e-27, 1.6058e-39, 3.0045e-11, 6.7065e-12, 2.2421e-44,\n",
      "         4.9900e-13, 1.6198e-25, 1.0563e-17, 5.5421e-31],\n",
      "        [7.9291e-11, 9.4186e-01, 4.9083e-03, 7.7542e-29, 3.8497e-12, 5.3079e-02,\n",
      "         2.9924e-21, 4.6591e-26, 9.3128e-11, 1.5567e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(8.2680e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor([[8.4080e-17, 9.9994e-01, 1.7332e-05, 1.0862e-09, 4.1009e-05, 1.1052e-07,\n",
      "         5.1830e-12, 3.0758e-33, 3.0114e-08, 1.1783e-22],\n",
      "        [2.8231e-05, 1.5864e-08, 1.8021e-22, 2.9651e-08, 9.9992e-01, 4.8604e-07,\n",
      "         3.1090e-25, 1.8341e-17, 2.4030e-16, 5.3148e-05],\n",
      "        [3.5260e-05, 3.9816e-05, 2.6402e-07, 1.3747e-08, 5.9073e-12, 1.8260e-07,\n",
      "         9.9992e-01, 3.8209e-06, 7.7438e-08, 8.5860e-29],\n",
      "        [2.2530e-05, 1.2875e-14, 9.9991e-01, 8.7594e-09, 4.9396e-05, 1.0416e-31,\n",
      "         1.3969e-05, 6.6293e-13, 1.0473e-07, 1.6160e-17],\n",
      "        [9.5548e-09, 6.2919e-05, 3.8568e-05, 2.2743e-27, 9.9989e-01, 3.5332e-07,\n",
      "         1.5149e-18, 1.5205e-24, 4.0157e-09, 5.8478e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(2.4247e-05, grad_fn=<NllLossBackward0>)\n",
      "tensor([[4.9619e-17, 9.9997e-01, 5.9415e-06, 6.1761e-10, 2.0960e-05, 5.7080e-08,\n",
      "         3.0854e-12, 1.8785e-33, 1.5381e-08, 7.1490e-23],\n",
      "        [6.7562e-06, 7.7870e-09, 8.1254e-23, 6.8651e-09, 9.9998e-01, 1.0895e-07,\n",
      "         1.7414e-25, 9.3642e-18, 1.2839e-16, 1.2189e-05],\n",
      "        [9.5729e-06, 1.0168e-05, 1.1920e-07, 4.2310e-09, 2.9388e-12, 5.4580e-08,\n",
      "         9.9998e-01, 1.5697e-06, 3.1999e-08, 3.6869e-29],\n",
      "        [9.1030e-06, 6.2317e-15, 9.9997e-01, 3.4230e-09, 1.3169e-05, 5.1357e-32,\n",
      "         5.0202e-06, 3.1481e-13, 3.4617e-08, 8.1098e-18],\n",
      "        [4.0031e-09, 1.5054e-05, 9.3520e-06, 9.6535e-28, 9.9997e-01, 1.1395e-07,\n",
      "         6.9091e-19, 6.7220e-25, 1.7569e-09, 1.9890e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(8.4400e-06, grad_fn=<NllLossBackward0>)\n",
      "tensor([[2.6274e-17, 9.9999e-01, 1.9169e-06, 3.0729e-10, 8.2882e-06, 2.5163e-08,\n",
      "         1.6459e-12, 1.0265e-33, 6.6791e-09, 3.8909e-23],\n",
      "        [2.2224e-06, 4.6280e-09, 4.3861e-23, 2.2308e-09, 9.9999e-01, 3.4480e-08,\n",
      "         1.1413e-25, 5.6332e-18, 8.0640e-17, 3.9282e-06],\n",
      "        [3.2153e-06, 3.3130e-06, 5.9988e-08, 1.4952e-09, 1.6041e-12, 1.9077e-08,\n",
      "         9.9999e-01, 7.0314e-07, 1.4351e-08, 1.7868e-29],\n",
      "        [3.5923e-06, 3.1112e-15, 9.9999e-01, 1.3435e-09, 4.1606e-06, 2.6189e-32,\n",
      "         1.8440e-06, 1.5492e-13, 1.2074e-08, 4.1755e-18],\n",
      "        [2.0141e-09, 4.9978e-06, 3.0558e-06, 4.9060e-28, 9.9999e-01, 4.3686e-08,\n",
      "         3.7310e-19, 3.5159e-25, 9.0391e-10, 7.9210e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "step: tensor(3.0756e-06, grad_fn=<NllLossBackward0>)\n",
      "tensor([[1.3704e-17, 1.0000e+00, 6.4151e-07, 1.4736e-10, 3.1028e-06, 1.0573e-08,\n",
      "         8.6572e-13, 5.5160e-34, 2.7676e-09, 2.0891e-23],\n",
      "        [7.9441e-07, 2.8941e-09, 2.4959e-23, 7.9478e-10, 1.0000e+00, 1.2011e-08,\n",
      "         7.8188e-26, 3.5341e-18, 5.2815e-17, 1.3877e-06],\n",
      "        [1.1476e-06, 1.1506e-06, 3.0723e-08, 5.4772e-10, 8.9397e-13, 6.9211e-09,\n",
      "         1.0000e+00, 3.1462e-07, 6.3793e-09, 8.8687e-30],\n",
      "        [1.3936e-06, 1.5728e-15, 1.0000e+00, 5.2452e-10, 1.4062e-06, 1.3541e-32,\n",
      "         6.8295e-07, 7.7159e-14, 4.3274e-09, 2.1798e-18],\n",
      "        [1.0599e-09, 1.7917e-06, 1.0799e-06, 2.6122e-28, 1.0000e+00, 1.7188e-08,\n",
      "         2.0974e-19, 1.9144e-25, 4.7844e-10, 3.1932e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x= torch.randn(5,784)\n",
    "w= torch.randn(10,784,requires_grad=True) \n",
    "#L层计算结果z\n",
    "z =x @ w.t()\n",
    "z.requires_grad_()\n",
    "print(z.size())\n",
    "#注意，这里没有使用one-hot编码\n",
    "y=torch.tensor([1,4,6,2,4])\n",
    "# Adam优化器\n",
    "optimizer =torch.optim.Adam([w],lr=1e-3)\n",
    "\n",
    "for step in range(10000):\n",
    "    #每次更新w，重新计算z\n",
    "    z = x @ w.t()\n",
    "    optimizer.zero_grad()\n",
    "    #先计算softmax的log，即-ylog(y_hat)中的log(y_hat)\n",
    "    pred = torch.log_softmax(z,dim=1)\n",
    "    # 然后再计算整个-ylog(y_hat),并在各class上求和，然后batch平均\n",
    "    loss = F.nll_loss(pred,y)\n",
    "    loss.backward(retain_graph=True)\n",
    "    #优化一次\n",
    "    optimizer.step()\n",
    "    if step % 2000 == 0:\n",
    "        print(\"step:\",loss)\n",
    "        #看看结果是否和标签一致\n",
    "        print(torch.softmax(z,dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daae79fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.15737595 0.06260203 0.99504317 0.71750219 0.08703786]\n",
      "   [0.62024848 0.9003287  0.94571371 0.22104428 0.74453719]\n",
      "   [0.00507831 0.4910834  0.85993882 0.33130481 0.53465971]\n",
      "   [0.24832634 0.54909834 0.59556805 0.54953646 0.027219  ]]\n",
      "\n",
      "  [[0.7685153  0.10671885 0.44421408 0.90997752 0.18812388]\n",
      "   [0.46093914 0.71341258 0.07852733 0.74360616 0.85632176]\n",
      "   [0.40963775 0.47276285 0.44205651 0.58594679 0.74053612]\n",
      "   [0.91930013 0.48555499 0.75415613 0.04752736 0.5892263 ]]\n",
      "\n",
      "  [[0.94153508 0.9560688  0.83756445 0.30346506 0.4581093 ]\n",
      "   [0.70544498 0.84513382 0.4518788  0.12336212 0.84693029]\n",
      "   [0.57933234 0.33458833 0.95423927 0.44083054 0.84886667]\n",
      "   [0.88444575 0.22161623 0.19499403 0.22234937 0.89492575]]]\n",
      "\n",
      "\n",
      " [[[0.73674011 0.64634938 0.4309061  0.40760771 0.92708374]\n",
      "   [0.13883415 0.92449871 0.65794137 0.00756057 0.86983562]\n",
      "   [0.14085854 0.84530683 0.7783565  0.76728577 0.14855727]\n",
      "   [0.37490832 0.97560236 0.28400691 0.19387647 0.59974549]]\n",
      "\n",
      "  [[0.97603292 0.61025335 0.79426374 0.87938338 0.40229405]\n",
      "   [0.7332704  0.52628372 0.85963734 0.34212373 0.01886269]\n",
      "   [0.83167091 0.21908712 0.0032458  0.91099841 0.03366935]\n",
      "   [0.15571417 0.81088026 0.42264581 0.73682671 0.16380606]]\n",
      "\n",
      "  [[0.33409111 0.97536951 0.57794448 0.35620704 0.64045079]\n",
      "   [0.1461913  0.15314321 0.43440022 0.40074134 0.98948237]\n",
      "   [0.81139689 0.14650775 0.14871898 0.3544597  0.83835657]\n",
      "   [0.20511809 0.92793331 0.5246693  0.31551992 0.65705781]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.15737595, 0.06260203, 0.99504317, 0.71750219, 0.08703786],\n",
       "        [0.62024848, 0.9003287 , 0.94571371, 0.22104428, 0.74453719],\n",
       "        [0.00507831, 0.4910834 , 0.85993882, 0.33130481, 0.53465971],\n",
       "        [0.24832634, 0.54909834, 0.59556805, 0.54953646, 0.027219  ]],\n",
       "\n",
       "       [[0.73674011, 0.64634938, 0.4309061 , 0.40760771, 0.92708374],\n",
       "        [0.13883415, 0.92449871, 0.65794137, 0.00756057, 0.86983562],\n",
       "        [0.14085854, 0.84530683, 0.7783565 , 0.76728577, 0.14855727],\n",
       "        [0.37490832, 0.97560236, 0.28400691, 0.19387647, 0.59974549]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 创建一个形状为 (2, 3, 4) 的三维数组\n",
    "arr = np.random.rand(2, 3, 4,5)\n",
    "print(arr)\n",
    "# 提取所有 batch 的第 0 个时间步的数据\n",
    "result = arr[:, 0]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e8be7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
